{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dff586a-1d45-4af9-8563-a0c6ae70d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71445914-efe8-4be7-b8af-1fc44649add2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a49fa8-a3d5-45ac-a6d2-242e0bb15885",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/cars\"\n",
    "data_path = Path(data_path)\n",
    "images_path = data_path / \"training_images\"\n",
    "annotations_file_path = data_path / \"annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a128c8d3-c5b0-4d09-8afd-a14da8f5fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_accelerated:Setting random seeds\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "from example.train_cars import load_cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe29fe6-d01f-44bb-ab6b-3edadb610984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, lookups = load_cars_df(annotations_file_path, images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0aac7-dc02-43c7-b4f6-3419758a0471",
   "metadata": {},
   "source": [
    "# Anchor section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8bf224-edc9-403d-a632-47c8646823e4",
   "metadata": {},
   "source": [
    "TODO: Now that we understand anchor boxes, lets look at how we can evaluate whether our chosen anchors are suitable for our problem and, if not, find some sensible choices for our dataset.\n",
    "\n",
    "The approach here is largely adapted from the autoanchor approach used in Yolov5, which was also used with Yolov7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640464e-1763-4dcf-b6c3-74136ef2a4eb",
   "metadata": {},
   "source": [
    "## Evaluating current anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d40c1-8c0e-4ce1-86b6-61485c43e1a0",
   "metadata": {},
   "source": [
    "The simplest approach would be to simply use the same anchors as used for COCO, which are already bundled with the defined architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87802e7d-09a5-45b3-854e-f6b0194f2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7 import create_yolov7_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cad1e97d-b3c9-45e8-917e-603d6efd3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_yolov7_model('yolov7', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5734996-5ad7-48ef-89b4-283e1341ed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[ 12.,  16.]]],\n",
       "\n",
       "\n",
       "          [[[ 19.,  36.]]],\n",
       "\n",
       "\n",
       "          [[[ 40.,  28.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 36.,  75.]]],\n",
       "\n",
       "\n",
       "          [[[ 76.,  55.]]],\n",
       "\n",
       "\n",
       "          [[[ 72., 146.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[142., 110.]]],\n",
       "\n",
       "\n",
       "          [[[192., 243.]]],\n",
       "\n",
       "\n",
       "          [[[459., 401.]]]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.detection_head.anchor_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4479730-9f9f-42cc-bdec-1dfe08d70f5f",
   "metadata": {},
   "source": [
    "By default these are the ones from coco. Here we can see that we have 3 groups, one for each layer of the feature pyramid network. The numbers correspond to the width and height of the anchors that will be generated.\n",
    "\n",
    "The FPN (Feature Pyramid Network) has three outputs and each output's role is to detect objects according to their scale. For example:\n",
    "\n",
    "- P3/8 is for detecting smaller objects.\n",
    "- P4/16 is for detecting medium objects.\n",
    "- P5/32 is for detecting bigger objects.\n",
    "So when you're going to detect smaller objects you need to use smaller anchor boxes and for medium objects you should use medium scale anchor boxes, so on\n",
    "\n",
    "TODO: Should have already explained what anchors are in a different section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05913eff-50df-4fd2-9743-53a120f9948c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12.,  16.],\n",
       "        [ 19.,  36.],\n",
       "        [ 40.,  28.],\n",
       "        [ 36.,  75.],\n",
       "        [ 76.,  55.],\n",
       "        [ 72., 146.],\n",
       "        [142., 110.],\n",
       "        [192., 243.],\n",
       "        [459., 401.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_anchors = model.detection_head.anchor_grid.clone().cpu().view(-1, 2); current_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22874ed5-545e-475e-80b3-e3b5f633719b",
   "metadata": {},
   "source": [
    "To evaluate our current anchor boxes, we can calculate the best possible recall, which would occur if the model was able to successfully match an appropriate anchor box with a ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848de0-e23f-41a7-a078-5b669e52d98d",
   "metadata": {},
   "source": [
    "### Find and Resize ground truth bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489722fc-f329-4754-9610-bcd32f927cca",
   "metadata": {},
   "source": [
    "To evaluate our anchor boxes, we first need some knowedge of the shapes and sizes of the objects in our dataset. However, before we can evaluate, we need to resize the width and height of our ground truth boxes based on the size of the images that we will train on - for this architecture, this is recommended to be 640.\n",
    "\n",
    "Let's start by finding the width and height of all ground truth boxes in the training set. We can calculate these as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "859fde1e-dfda-4891-b2c2-c7e61d95c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df = train_df.query('has_annotation == True').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8b32f6-32e7-4ea8-9ea4-735024c4156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df['h'] = train_annotations_df['ymax'] -  train_annotations_df['ymin']\n",
    "train_annotations_df['w'] = train_annotations_df['xmax'] -  train_annotations_df['xmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5f14ece-bf59-4661-8827-2997141ed2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>has_annotation</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>281.259045</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>327.727931</td>\n",
       "      <td>223.225547</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.190476</td>\n",
       "      <td>46.468886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>15.163531</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>120.329957</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.395109</td>\n",
       "      <td>105.166425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>239.192475</td>\n",
       "      <td>176.764801</td>\n",
       "      <td>361.968162</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.665380</td>\n",
       "      <td>122.775687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>16.630970</td>\n",
       "      <td>186.546010</td>\n",
       "      <td>132.558611</td>\n",
       "      <td>238.386422</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.840412</td>\n",
       "      <td>115.927641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vid_4_10100.jpg</td>\n",
       "      <td>447.568741</td>\n",
       "      <td>160.625804</td>\n",
       "      <td>582.083936</td>\n",
       "      <td>232.517696</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.891892</td>\n",
       "      <td>134.515195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>vid_4_9860.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.321729</td>\n",
       "      <td>49.235251</td>\n",
       "      <td>236.223284</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.901554</td>\n",
       "      <td>49.235251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>vid_4_9880.jpg</td>\n",
       "      <td>329.876184</td>\n",
       "      <td>156.482351</td>\n",
       "      <td>536.664239</td>\n",
       "      <td>250.497895</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.015544</td>\n",
       "      <td>206.788055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>vid_4_9900.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.295823</td>\n",
       "      <td>141.797524</td>\n",
       "      <td>239.176652</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.880829</td>\n",
       "      <td>141.797524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>vid_4_9960.jpg</td>\n",
       "      <td>487.428988</td>\n",
       "      <td>172.233646</td>\n",
       "      <td>616.917699</td>\n",
       "      <td>228.839864</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.606218</td>\n",
       "      <td>129.488711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>vid_4_9980.jpg</td>\n",
       "      <td>221.558631</td>\n",
       "      <td>182.570434</td>\n",
       "      <td>348.585579</td>\n",
       "      <td>238.192196</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.621762</td>\n",
       "      <td>127.026948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image        xmin        ymin        xmax        ymax  \\\n",
       "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547   \n",
       "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180   \n",
       "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180   \n",
       "4    vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422   \n",
       "5    vid_4_10100.jpg  447.568741  160.625804  582.083936  232.517696   \n",
       "..               ...         ...         ...         ...         ...   \n",
       "554   vid_4_9860.jpg    0.000000  198.321729   49.235251  236.223284   \n",
       "555   vid_4_9880.jpg  329.876184  156.482351  536.664239  250.497895   \n",
       "556   vid_4_9900.jpg    0.000000  168.295823  141.797524  239.176652   \n",
       "557   vid_4_9960.jpg  487.428988  172.233646  616.917699  228.839864   \n",
       "558   vid_4_9980.jpg  221.558631  182.570434  348.585579  238.192196   \n",
       "\n",
       "    class_name  has_annotation  image_id  class_id          h           w  \n",
       "0          car            True         0       0.0  36.190476   46.468886  \n",
       "1          car            True         1       0.0  49.395109  105.166425  \n",
       "2          car            True         3       0.0  59.665380  122.775687  \n",
       "4          car            True         4       0.0  51.840412  115.927641  \n",
       "5          car            True         6       0.0  71.891892  134.515195  \n",
       "..         ...             ...       ...       ...        ...         ...  \n",
       "554        car            True       994       0.0  37.901554   49.235251  \n",
       "555        car            True       995       0.0  94.015544  206.788055  \n",
       "556        car            True       996       0.0  70.880829  141.797524  \n",
       "557        car            True       999       0.0  56.606218  129.488711  \n",
       "558        car            True      1000       0.0  55.621762  127.026948  \n",
       "\n",
       "[397 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8cd26dd-b209-4e17-ac3c-31293d05f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gt_wh = train_annotations_df[['w', 'h']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f6006-f5c6-4d11-83b8-594f00d5bdb8",
   "metadata": {},
   "source": [
    "Next, we will need the height and width of our images. Sometimes, we have this information ahead of time, in which case we can use this knowledge directly. Otherwise, We can do this as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9975c3ff-ea78-4975-a327-93214babfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07604707-ac4c-47b0-88e6-81f1e94e84be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 324/324 [00:00<00:00, 5619.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def find_image_size(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    w, h = image.size\n",
    "    return (image_path.parts[-1], (w, h))\n",
    "\n",
    "image_sizes = process_map(find_image_size, [images_path/p for p in train_df.image.unique()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5813a3db-fe11-4fe8-b332-cf75be4690f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes_df = pd.DataFrame(dict(image_sizes)).T.reset_index().rename(columns={'index': 'image', 0: 'image_w', 1:'image_h'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebe1b49b-e4f7-42c5-9c94-b6210ffc3e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>image_w</th>\n",
       "      <th>image_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10100.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>vid_4_13060.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>vid_4_13100.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>vid_4_13240.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>vid_4_13280.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>vid_4_13300.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image  image_w  image_h\n",
       "0     vid_4_1000.jpg      676      380\n",
       "1    vid_4_10000.jpg      676      380\n",
       "2    vid_4_10040.jpg      676      380\n",
       "3    vid_4_10060.jpg      676      380\n",
       "4    vid_4_10100.jpg      676      380\n",
       "..               ...      ...      ...\n",
       "319  vid_4_13060.jpg      676      380\n",
       "320  vid_4_13100.jpg      676      380\n",
       "321  vid_4_13240.jpg      676      380\n",
       "322  vid_4_13280.jpg      676      380\n",
       "323  vid_4_13300.jpg      676      380\n",
       "\n",
       "[324 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sizes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463e798-43ef-4201-b06b-f0d7f3bc0a77",
   "metadata": {},
   "source": [
    "We can now merge this with our existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6e93b78-4c00-47e1-a4ab-1c18140adef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>has_annotation</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>image_w</th>\n",
       "      <th>image_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>281.259045</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>327.727931</td>\n",
       "      <td>223.225547</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.190476</td>\n",
       "      <td>46.468886</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>15.163531</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>120.329957</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.395109</td>\n",
       "      <td>105.166425</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>239.192475</td>\n",
       "      <td>176.764801</td>\n",
       "      <td>361.968162</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.665380</td>\n",
       "      <td>122.775687</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>16.630970</td>\n",
       "      <td>186.546010</td>\n",
       "      <td>132.558611</td>\n",
       "      <td>238.386422</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.840412</td>\n",
       "      <td>115.927641</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10100.jpg</td>\n",
       "      <td>447.568741</td>\n",
       "      <td>160.625804</td>\n",
       "      <td>582.083936</td>\n",
       "      <td>232.517696</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.891892</td>\n",
       "      <td>134.515195</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>vid_4_9860.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.321729</td>\n",
       "      <td>49.235251</td>\n",
       "      <td>236.223284</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.901554</td>\n",
       "      <td>49.235251</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>vid_4_9880.jpg</td>\n",
       "      <td>329.876184</td>\n",
       "      <td>156.482351</td>\n",
       "      <td>536.664239</td>\n",
       "      <td>250.497895</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.015544</td>\n",
       "      <td>206.788055</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>vid_4_9900.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.295823</td>\n",
       "      <td>141.797524</td>\n",
       "      <td>239.176652</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.880829</td>\n",
       "      <td>141.797524</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>vid_4_9960.jpg</td>\n",
       "      <td>487.428988</td>\n",
       "      <td>172.233646</td>\n",
       "      <td>616.917699</td>\n",
       "      <td>228.839864</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.606218</td>\n",
       "      <td>129.488711</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>vid_4_9980.jpg</td>\n",
       "      <td>221.558631</td>\n",
       "      <td>182.570434</td>\n",
       "      <td>348.585579</td>\n",
       "      <td>238.192196</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.621762</td>\n",
       "      <td>127.026948</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image        xmin        ymin        xmax        ymax  \\\n",
       "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547   \n",
       "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180   \n",
       "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180   \n",
       "3    vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422   \n",
       "4    vid_4_10100.jpg  447.568741  160.625804  582.083936  232.517696   \n",
       "..               ...         ...         ...         ...         ...   \n",
       "392   vid_4_9860.jpg    0.000000  198.321729   49.235251  236.223284   \n",
       "393   vid_4_9880.jpg  329.876184  156.482351  536.664239  250.497895   \n",
       "394   vid_4_9900.jpg    0.000000  168.295823  141.797524  239.176652   \n",
       "395   vid_4_9960.jpg  487.428988  172.233646  616.917699  228.839864   \n",
       "396   vid_4_9980.jpg  221.558631  182.570434  348.585579  238.192196   \n",
       "\n",
       "    class_name  has_annotation  image_id  class_id          h           w  \\\n",
       "0          car            True         0       0.0  36.190476   46.468886   \n",
       "1          car            True         1       0.0  49.395109  105.166425   \n",
       "2          car            True         3       0.0  59.665380  122.775687   \n",
       "3          car            True         4       0.0  51.840412  115.927641   \n",
       "4          car            True         6       0.0  71.891892  134.515195   \n",
       "..         ...             ...       ...       ...        ...         ...   \n",
       "392        car            True       994       0.0  37.901554   49.235251   \n",
       "393        car            True       995       0.0  94.015544  206.788055   \n",
       "394        car            True       996       0.0  70.880829  141.797524   \n",
       "395        car            True       999       0.0  56.606218  129.488711   \n",
       "396        car            True      1000       0.0  55.621762  127.026948   \n",
       "\n",
       "     image_w  image_h  \n",
       "0        676      380  \n",
       "1        676      380  \n",
       "2        676      380  \n",
       "3        676      380  \n",
       "4        676      380  \n",
       "..       ...      ...  \n",
       "392      676      380  \n",
       "393      676      380  \n",
       "394      676      380  \n",
       "395      676      380  \n",
       "396      676      380  \n",
       "\n",
       "[397 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_df = pd.merge(train_annotations_df, image_sizes_df, on='image'); train_annotations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ecde5a2-72ad-44d5-8b34-f7fcb503195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = train_annotations_df[['image_w', 'image_h']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcc39b-f8af-4bd2-9e2c-1cbbc5e61484",
   "metadata": {},
   "source": [
    "Now, we can use this information to get the resized widths and heights of our ground truth targets, with respect to our target image size. To preserve the aspect ratios of the objects in our images, the recommended approach to resizing is to scale the image so that the longest size is equal to our target size. We can do this using the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840356b3-6f84-4439-84cb-8c2578895ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.anchors import calculate_resized_gt_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2de23342-de90-4d3e-983a-d0dee40f98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mcalculate_resized_gt_wh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mcalculate_resized_gt_wh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_image_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# image sizes array of [w, h] , either np.array([[w, h]]) or per image\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalized_gt_wh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_wh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimage_sizes\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# find target image sizes, assuming resizing so that the longest side is the target size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_image_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtarget_image_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_sizes\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimage_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# find wh of boxes for target size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_image_sizes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnormalized_gt_wh\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtiny_boxes_exist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwh\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtiny_boxes_exist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"WARNING: Extremely small objects found. {tiny_boxes_exist} of {len(wh)} labels are < 3 pixels in size.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwh\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# filter > 2 pixels\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/yolov7/anchors.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??calculate_resized_gt_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cdebb45b-c34e-442b-babd-3f4a7955a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_gt_wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4533c24a-0c30-4c70-84c2-b6e5d678e8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43.99421122,  34.26317273],\n",
       "       [ 99.56584662,  46.76460062],\n",
       "       [116.23733718,  56.48793344],\n",
       "       [109.75397973,  49.07967981],\n",
       "       [127.35166419,  68.06332961]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_wh = calculate_resized_gt_wh(raw_gt_wh, image_sizes, target_image_size=640); gt_wh[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e9ad9-78bd-48c0-a541-335a6171b33b",
   "metadata": {},
   "source": [
    "Alternatively, as all of our images are the same size in this case, we could simply specify a single image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4a3e8ce-05b7-49cc-a1d4-c4e2dee31d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43.99421122,  34.26317273],\n",
       "       [ 99.56584662,  46.76460062],\n",
       "       [116.23733718,  56.48793344],\n",
       "       [109.75397973,  49.07967981],\n",
       "       [127.35166419,  68.06332961]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_resized_gt_wh(raw_gt_wh, image_sizes=np.array([[676, 380]]), target_image_size=640)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc48f6-c153-43ef-91ea-8bca2589c2af",
   "metadata": {},
   "source": [
    "Note that we have also filtered out any boxes what will be incredibly small (less than 3 pixels in either height or width), with respect to the new image size, as these boxes are usually too small to be considered useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369121eb-20a8-4585-99eb-6e617e0ef89d",
   "metadata": {},
   "source": [
    "### Calculating Best Possible Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407008e9-789b-4c16-b27c-95ed64e0d19c",
   "metadata": {},
   "source": [
    "Now that we have the width and height of all ground truth boxes in our training set, we can evaluate our current anchor boxes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39428879-6673-4e1b-9829-afcbada0f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.anchors import calculate_best_possible_recall, LOSS_ANCHOR_MULTIPLE_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e44b584-a58b-4f70-992e-b4c6436f0219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mcalculate_best_possible_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mcalculate_best_possible_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Given a tensor of anchors and and an array of widths and heights for each bounding box in the dataset,\u001b[0m\n",
       "\u001b[0;34m    calculate the best possible recall that can be obtained if every box was matched to an appropriate anchor.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    :param anchors: a tensor of shape [N, 2] representing the width and height of each anchor\u001b[0m\n",
       "\u001b[0;34m    :param gt_wh: a tensor of shape [N, 2] representing the width and height of each ground truth bounding box\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_anchor_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_best_anchor_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_possible_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m(\u001b[0m\u001b[0mbest_anchor_ratio\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mLOSS_ANCHOR_MULTIPLE_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mbest_possible_recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/yolov7/anchors.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??calculate_best_possible_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "233fe07b-d946-48b2-9d8f-1d19cd83855c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_best_possible_recall(current_anchors, gt_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46907396-a5ba-4d50-a3cb-9c9c0075974b",
   "metadata": {},
   "source": [
    "From this, we can see that the current anchor boxes are a good fit for this dataset; which makes sense, as the images are quite similar to those in COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf91ad-3654-47d7-9c35-b6bdb1137841",
   "metadata": {},
   "source": [
    "### How does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a3975-ab1c-4f16-88aa-1cbffb07113b",
   "metadata": {},
   "source": [
    "At this point, you may be wondering, how exactly do we calculate the best possible recall. To answer this, let's go through the process manually.\n",
    "\n",
    "\n",
    "Intuitively, we would like to ensure that at least one anchor can be matched to each ground truth box. Whilst we could do this by framing it as an optimization problem - how do we match each ground truth box with its optimal anchor - this would introduce a lot of complexity for what we are trying to do.\n",
    "\n",
    "Given an anchor box, we need a simpler way of measuring how well it can be made to fit a ground truth box. Let's examine one approach that can be taken to do this, starting with the width and height of a single ground truth box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b529074b-3588-4fcb-bd6e-fa4770a94d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.99421122, 34.26317273])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_box_wh = gt_wh[0]; gt_box_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bba7a2-994f-4b70-a864-cbba735c6ee2",
   "metadata": {},
   "source": [
    "For each anchor box, we can inspect the ratios of its height and width when compared to the height and width of our ground truth target, and use this to understand where the biggest differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06ab7b8b-fbdd-4af5-b34a-ef44249ea260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2728,  0.4670],\n",
       "        [ 0.4319,  1.0507],\n",
       "        [ 0.9092,  0.8172],\n",
       "        [ 0.8183,  2.1889],\n",
       "        [ 1.7275,  1.6052],\n",
       "        [ 1.6366,  4.2611],\n",
       "        [ 3.2277,  3.2104],\n",
       "        [ 4.3642,  7.0922],\n",
       "        [10.4332, 11.7035]], dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_anchors/gt_box_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8385ba-39e7-4994-a656-d9195d13bed0",
   "metadata": {},
   "source": [
    "As the scale of these ratios will depend on whether the anchor box sides are greater or smaller than the sides of our ground truth box, we can ensure that our magnitudes are in the range [0, 1] by also calculating the reciprocal and taking the minimum ratios for each anchor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6042b81-de5a-42a7-a66c-c7d084f5b932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2728, 0.4670],\n",
       "        [0.4319, 0.9518],\n",
       "        [0.9092, 0.8172],\n",
       "        [0.8183, 0.4568],\n",
       "        [0.5789, 0.6230],\n",
       "        [0.6110, 0.2347],\n",
       "        [0.3098, 0.3115],\n",
       "        [0.2291, 0.1410],\n",
       "        [0.0958, 0.0854]], dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symmetric_size_ratios = torch.min(current_anchors/gt_box_wh, gt_box_wh/current_anchors); symmetric_size_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7d3ac-fc3f-499a-9189-59b42adce5a3",
   "metadata": {},
   "source": [
    "From this, we now have an indication of how well, independently, the width and height of each anchor box 'fits' to our ground truth target. \n",
    "\n",
    "Now, our challenge is how to evaluate the matching of the the width and height together!\n",
    "\n",
    "One way we can approach this is, to take the minimum ratio for each anchor; representing the side that worst matches our ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7a1e5c0-71ed-465e-813e-30f9feb9995e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2728, 0.4319, 0.8172, 0.4568, 0.5789, 0.2347, 0.3098, 0.1410, 0.0854],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_side_size_ratio = symmetric_size_ratios.min(-1).values; worst_side_size_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c655fb-a746-43f0-a217-4d74b075b46c",
   "metadata": {},
   "source": [
    "The reason why we have selected the worst fitting side here, is because we know that the other side matches our target *at least* as well as the one selected; we can think of this as the worst case scenario!\n",
    "\n",
    "Now, let's select the anchor box which matches the best out of these options, this is simply the largest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5fead607-04be-4537-b47b-f021147f184d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8172, dtype=torch.float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_anchor_ratio = worst_side_size_ratio.max(-1).values; best_anchor_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf4414-92fd-43fd-b4c6-736370e524bf",
   "metadata": {},
   "source": [
    "Out of the worst fitting options, this is our selected match!\n",
    "\n",
    "\n",
    "TODO: Ensure this is after the loss function section\n",
    "\n",
    "Recalling that the loss function only looks to match anchor boxes that are up to 4 times greater or smaller than the size of the ground truth target, we can now verify whether this anchor is within this range and would be considered a successful match.\n",
    "\n",
    "We can do that as demonstrated below, taking the reciprical of our loss multiple, to ensure that it is in the same range as our value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f260ee0f-6ddc-4c52-a8b0-5014ec3ad8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOSS_ANCHOR_MULTIPLE_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "950240a5-eeae-4141-bb23-6bd145e7bf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_anchor_ratio > 1. / LOSS_ANCHOR_MULTIPLE_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de729b8-6bd7-450d-9668-35999b2c90af",
   "metadata": {},
   "source": [
    "From this, we can see that at least one of our anchors could be successfully matched to our selected ground truth target!\n",
    "\n",
    "Now that we understand the sequence of steps, we can now apply the same logic to all of our ground truth boxes to see how many matches we can obtain with our current set of anchors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "feab2b68-348c-4c64-8ff4-4a4a92d322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_size_ratios = torch.min(current_anchors[None]/gt_wh[:, None],\n",
    "                                  gt_wh[:, None]/current_anchors[None])\n",
    "worst_side_size_ratio = symmetric_size_ratios.min(-1).values\n",
    "best_anchor_ratio = worst_side_size_ratio.max(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e5240a5-2d88-4ed9-a483-7501790d804a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_anchor_ratio > 1. / LOSS_ANCHOR_MULTIPLE_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0c5d5-8c56-4907-85c8-8f48bd1f1e46",
   "metadata": {},
   "source": [
    "Now that we have calculated, for each ground truth box, whether it has a match. We can take the mean number of matches to find out best possible recall; in our case, this is 1, as we saw earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33a55c5f-cb1b-4003-9abc-c04ca1ddf384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_possible_recall = (best_anchor_ratio > 1. / LOSS_ANCHOR_MULTIPLE_THRESHOLD).float().mean(); best_possible_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13a225-a0f7-4f3a-a69e-3766628eeb0d",
   "metadata": {},
   "source": [
    "## Selecting new anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625a535-6c8a-46ff-99c5-54cf4ea470f3",
   "metadata": {},
   "source": [
    "Whilst using the pre-defined anchors may be a good choice for similar datasets, this may not be appropriate for all datasets, for example, those that contain lots of small objects. In these cases, a better approach may be to select entirely new anchors.\n",
    "\n",
    "Let's explore how we can do this!\n",
    "\n",
    "First, let's define the number of anchors that we need for our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0dbf0cc8-9453-4460-8ea7-b3866767f036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_anchors = current_anchors.shape[0]; num_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f2423-992b-4b99-8571-a6eb68934ef4",
   "metadata": {},
   "source": [
    "Now, based on our bounding boxes, we need to define a sensible set widths and heights of anchor templates. One way that we can estimate this is by using Kmeans to cluster our ground truth aspect ratios, based on the number of anchors that we need. We can then use these centroids as our starting estimates. We can do this using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c054506-171c-498c-a1e0-ba88e3b0af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.anchors import estimate_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26b92d80-45c3-4174-be50-520261229142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mestimate_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mestimate_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Given a target number of anchors and an array of widths and heights for each bounding box in the dataset,\u001b[0m\n",
       "\u001b[0;34m    estimate a set of anchors using the centroids from Kmeans clustering.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    :param num_anchors: the number of anchors to return\u001b[0m\n",
       "\u001b[0;34m    :param gt_wh: an array of shape [N, 2] representing the width and height of each ground truth bounding box\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running kmeans for {num_anchors} anchors on {len(gt_wh)} points...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstd_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproposed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mgt_wh\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m  \u001b[0;31m# divide by std so they are in approx same range\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproposed_anchors\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mstd_dev\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mproposed_anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/yolov7/anchors.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??estimate_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e66b7dc-0b74-40d6-a371-c79ea2cad532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running kmeans for 9 anchors on 397 points...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[157.29889337,  57.47936534],\n",
       "       [ 70.37782144,  28.9259909 ],\n",
       "       [117.67344588,  55.91906451],\n",
       "       [ 71.90866699,  50.8658278 ],\n",
       "       [186.28917826,  84.09961313],\n",
       "       [ 38.3042379 ,  25.61764762],\n",
       "       [ 56.74676871,  36.96437289],\n",
       "       [112.83692506,  42.92073289],\n",
       "       [ 90.20439918,  36.43405811]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposed_anchors = estimate_anchors(num_anchors, gt_wh); proposed_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5931060-0e6c-44a8-905f-86c0d6fa9cd9",
   "metadata": {},
   "source": [
    "Here, we can see that we now have a set of anchor templates that we can use as a starting point. As before, let's calculate our best possible recall using these anchors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ce1c5f08-ab53-4ee6-8d04-cd4169659b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_best_possible_recall(proposed_anchors, gt_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4d854-671a-4b29-bc91-b65cc4ab8bf9",
   "metadata": {},
   "source": [
    "Once again, we see that our best possible recall is 1, which means that these anchors are also a good fit for our problem!\n",
    "\n",
    "Whilst it is perhaps unnecessary in this case, we may be able improve these anchors further using a [genetic algorithm](https://www.geeksforgeeks.org/genetic-algorithms/). Following this methodology, we can define a *fitness* (or reward) function to measure how well our anchors match our data and make small, random changes to our anchors to try and maximise this function. \n",
    "\n",
    "In this case we can define our fitness function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38382672-e45e-4a41-854d-50eb22c11d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.anchors import anchor_fitness, evolve_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2998510d-568c-4c17-9ae3-6104893085cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0manchor_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0manchor_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    A fitness function that can be used to evolve a set of anchors. This function calculates the mean best anchor ratio\u001b[0m\n",
       "\u001b[0;34m    for all matches that are within the multiple range considered during the loss calculation.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_anchor_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_best_anchor_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbest_anchor_ratio\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbest_anchor_ratio\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mLOSS_ANCHOR_MULTIPLE_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/yolov7/anchors.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??anchor_fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57254677-7314-4d0b-bdef-1983a966ba53",
   "metadata": {},
   "source": [
    "Here, we are taking the best anchor ratio for each match that will be considered during the loss calculation. If an anchor box is more than four times greater or smaller than its matched bounding box, it will not contribute to our score. Let's use this to calculate a fitness score for our proposed anchors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c783791a-1799-46f7-91aa-bfc803ec27ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8825, dtype=torch.float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_fitness(proposed_anchors, gt_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26fe15-7676-4403-8590-0aabe4aba3f8",
   "metadata": {},
   "source": [
    " Now, let's use this as the fitness function when optimizing our anchors, as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0f36a42d-9c81-484d-89c6-1ebed5ce2f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evolving anchors with Genetic Algorithm: fitness = 0.8855: 100%|█| 30000/30000 [00:19<00:00,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[156.06907735,  61.09621462],\n",
       "       [ 66.80622862,  29.0438958 ],\n",
       "       [136.33063134,  52.56100946],\n",
       "       [ 80.06700492,  36.42821897],\n",
       "       [179.52356295,  83.0822995 ],\n",
       "       [ 37.64336168,  29.35155407],\n",
       "       [ 52.00907081,  37.50677223],\n",
       "       [114.96001811,  44.23679448],\n",
       "       [ 99.66602472,  37.86939469]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evolved_anchors = evolve_anchors(proposed_anchors, gt_wh, anchor_fitness_fn=anchor_fitness, num_iterations=30000); evolved_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee00d0-ab5a-46b6-a793-7327e63befd6",
   "metadata": {},
   "source": [
    "Inspecting the definition of this function, we can see that, for a specified number of iterations, we are simply sampling random noise from a normal distribution and using this to mutate our anchors. If this change leads to an increased score, we keep these as our anchors! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14e7c8f2-540f-48ad-89ba-65c317f6969b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mevolve_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproposed_anchors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgt_wh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmutation_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmutation_noise_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmutation_noise_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manchor_fitness_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0manchor_fitness\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f17b52be5f0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mevolve_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproposed_anchors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgt_wh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmutation_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmutation_noise_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmutation_noise_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manchor_fitness_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manchor_fitness\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Use a genetic algorithm to mutate the given anchors to try and optimise them based on the given widths and heights of the\u001b[0m\n",
       "\u001b[0;34m    ground truth boxes based on the provided fitness function. Anchor dimensions are mutated by adding random noise sampled\u001b[0m\n",
       "\u001b[0;34m    from a normal distribution with the mean and standard deviation provided.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    :param proposed_anchors: a tensor containing the aspect ratios of the anchor boxes to evolve\u001b[0m\n",
       "\u001b[0;34m    :param gt_wh: a tensor of shape [N, 2] representing the width and height of each ground truth bounding box\u001b[0m\n",
       "\u001b[0;34m    :param num_generations: the number of iterations for which to run the algorithm\u001b[0m\n",
       "\u001b[0;34m    :param mutation_probability: the probability that each anchor dimension is mutated during each iteration\u001b[0m\n",
       "\u001b[0;34m    :param mutation_noise_mean: the mean of the normal distribution from which the mutation noise will be sampled\u001b[0m\n",
       "\u001b[0;34m    :param mutation_noise_std: the standard deviation of the normal distribution from which the mutation noise will be sampled\u001b[0m\n",
       "\u001b[0;34m    :param anchor_fitness_fn: the reward function that will be used during the optimization process. This should accept proposed_anchors and gt_wh as arguments\u001b[0m\n",
       "\u001b[0;34m    :param verbose: if True, the value of the fitness function will be printed at the end of each iteration\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_fitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchor_fitness_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manchor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposed_anchors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Evolving anchors with Genetic Algorithm:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Define mutation by sampling noise from a normal distribution \u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0manchor_mutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0manchor_mutation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmutation_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchor_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m*\u001b[0m \u001b[0mmutation_noise_std\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m+\u001b[0m \u001b[0mmutation_noise_mean\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmutated_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproposed_anchors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0manchor_mutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmutated_anchor_fitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchor_fitness_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutated_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_wh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mmutated_anchor_fitness\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_fitness\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbest_fitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposed_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mmutated_anchor_fitness\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mmutated_anchors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Evolving anchors with Genetic Algorithm: fitness = {best_fitness:.4f}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration: {i}, Fitness: {best_fitness}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mproposed_anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/yolov7/anchors.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??evolve_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf6bd4-0eb6-4a85-bec7-d545fbcab42a",
   "metadata": {},
   "source": [
    "Let's see whether this has improved our score at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c9f2dce8-2d9a-4fbc-ab38-0b5c18dc2d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8876, dtype=torch.float64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_fitness(evolved_anchors, gt_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfed87-bf6b-430d-bc7e-b1bfe931598a",
   "metadata": {},
   "source": [
    "We can see that our evolved anchors have a better fitness score than our original proposed anchors, as we would expect!\n",
    "\n",
    "Now, all that is left to do is to sort the anchors into a rough ascending order, considering the smallest dimension for each anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1903f7fa-10c7-4adf-afe5-757ce6785038",
   "metadata": {},
   "outputs": [],
   "source": [
    "evolved_anchors = torch.as_tensor(evolved_anchors)[torch.sort(torch.as_tensor(evolved_anchors.min(-1))).indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b8ab88b-b716-44b5-a533-0c63deb30b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_best_possible_recall(evolved_anchors, gt_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b4c88-2e77-4c5f-8d56-d0e7b307e6e1",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33424df0-81d9-4642-8ff5-ff433f07a952",
   "metadata": {},
   "source": [
    "Now that we understand the process, we could calculate our anchors for our dataset in a single step using the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cc89707e-5d66-472e-9e04-6b17e726edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.anchors import calculate_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e92bbefc-eee0-4c9e-abd3-1dc3a1984f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Possible Recall (BPR) = 1.0000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 12.,  16.],\n",
       "        [ 19.,  36.],\n",
       "        [ 40.,  28.],\n",
       "        [ 36.,  75.],\n",
       "        [ 76.,  55.],\n",
       "        [ 72., 146.],\n",
       "        [142., 110.],\n",
       "        [192., 243.],\n",
       "        [459., 401.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_anchors(current_anchors, image_sizes, gt_wh, target_image_size=640, best_possible_recall_threshold=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1ded5-cd99-4281-b672-18043f40bdef",
   "metadata": {},
   "source": [
    "In this case, as our best possible recall is already greater than the threshold, we can keep our original anchors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5deea-bf98-40c5-9424-c3172a8932ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
