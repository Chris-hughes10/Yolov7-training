{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff586a-1d45-4af9-8563-a0c6ae70d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71445914-efe8-4be7-b8af-1fc44649add2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a49fa8-a3d5-45ac-a6d2-242e0bb15885",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/cars\"\n",
    "data_path = Path(data_path)\n",
    "images_path = data_path / \"training_images\"\n",
    "annotations_file_path = data_path / \"annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128c8d3-c5b0-4d09-8afd-a14da8f5fe2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_accelerated:Setting random seeds\n"
     ]
    }
   ],
   "source": [
    "from example.train_cars import load_cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe29fe6-d01f-44bb-ab6b-3edadb610984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, lookups = load_cars_df(annotations_file_path, images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0aac7-dc02-43c7-b4f6-3419758a0471",
   "metadata": {},
   "source": [
    "# Anchor section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8bf224-edc9-403d-a632-47c8646823e4",
   "metadata": {},
   "source": [
    "TODO: Now that we understand anchor boxes, lets look at how we can evaluate whether our chosen anchors are suitable for our problem and, if not, find some sensible choices for our dataset.\n",
    "\n",
    "The approach here is largely adapted from the autoanchor approach used in Yolov5, which was also used with Yolov7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640464e-1763-4dcf-b6c3-74136ef2a4eb",
   "metadata": {},
   "source": [
    "## Evaluating current anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d40c1-8c0e-4ce1-86b6-61485c43e1a0",
   "metadata": {},
   "source": [
    "The simplest approach would be to simply use the same anchors as used for COCO, which are already bundled with the defined architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "87802e7d-09a5-45b3-854e-f6b0194f2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7 import create_yolov7_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cad1e97d-b3c9-45e8-917e-603d6efd3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_yolov7_model('yolov7', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e5734996-5ad7-48ef-89b4-283e1341ed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[ 12.,  16.]]],\n",
       "\n",
       "\n",
       "          [[[ 19.,  36.]]],\n",
       "\n",
       "\n",
       "          [[[ 40.,  28.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[ 36.,  75.]]],\n",
       "\n",
       "\n",
       "          [[[ 76.,  55.]]],\n",
       "\n",
       "\n",
       "          [[[ 72., 146.]]]]],\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        [[[[[142., 110.]]],\n",
       "\n",
       "\n",
       "          [[[192., 243.]]],\n",
       "\n",
       "\n",
       "          [[[459., 401.]]]]]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.detection_head.anchor_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4479730-9f9f-42cc-bdec-1dfe08d70f5f",
   "metadata": {},
   "source": [
    "By default these are the ones from coco. Here we can see that we have 3 groups, one for each layer of the feature pyramid network. The numbers correspond to the width and height of the anchors that will be generated.\n",
    "\n",
    "The FPN (Feature Pyramid Network) has three outputs and each output's role is to detect objects according to their scale. For example:\n",
    "\n",
    "- P3/8 is for detecting smaller objects.\n",
    "- P4/16 is for detecting medium objects.\n",
    "- P5/32 is for detecting bigger objects.\n",
    "So when you're going to detect smaller objects you need to use smaller anchor boxes and for medium objects you should use medium scale anchor boxes, so on\n",
    "\n",
    "TODO: Should have already explained what anchors are in a different section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "05913eff-50df-4fd2-9743-53a120f9948c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 12.,  16.],\n",
       "        [ 19.,  36.],\n",
       "        [ 40.,  28.],\n",
       "        [ 36.,  75.],\n",
       "        [ 76.,  55.],\n",
       "        [ 72., 146.],\n",
       "        [142., 110.],\n",
       "        [192., 243.],\n",
       "        [459., 401.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_anchors = model.detection_head.anchor_grid.clone().cpu().view(-1, 2); current_anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22874ed5-545e-475e-80b3-e3b5f633719b",
   "metadata": {},
   "source": [
    "To evaluate our current anchor boxes, we can calculate the best possible recall, which would occur if the model was able to successfully match an appropriate anchor box with a ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848de0-e23f-41a7-a078-5b669e52d98d",
   "metadata": {},
   "source": [
    "### Find normalized bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489722fc-f329-4754-9610-bcd32f927cca",
   "metadata": {},
   "source": [
    "To evaluate our anchor boxes, we first need some knowedge of the shapes and sizes of the objects in our dataset. We can do this by finding the width and height of all ground truth boxes in the training set. We can calculate these as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "859fde1e-dfda-4891-b2c2-c7e61d95c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df = train_df.query('has_annotation == True').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c8b32f6-32e7-4ea8-9ea4-735024c4156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df['h'] = train_annotations_df['ymax'] -  train_annotations_df['ymin']\n",
    "train_annotations_df['w'] = train_annotations_df['xmax'] -  train_annotations_df['xmin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5f14ece-bf59-4661-8827-2997141ed2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>has_annotation</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>281.259045</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>327.727931</td>\n",
       "      <td>223.225547</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.190476</td>\n",
       "      <td>46.468886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>15.163531</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>120.329957</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.395109</td>\n",
       "      <td>105.166425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>239.192475</td>\n",
       "      <td>176.764801</td>\n",
       "      <td>361.968162</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.665380</td>\n",
       "      <td>122.775687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>16.630970</td>\n",
       "      <td>186.546010</td>\n",
       "      <td>132.558611</td>\n",
       "      <td>238.386422</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.840412</td>\n",
       "      <td>115.927641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vid_4_10100.jpg</td>\n",
       "      <td>447.568741</td>\n",
       "      <td>160.625804</td>\n",
       "      <td>582.083936</td>\n",
       "      <td>232.517696</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.891892</td>\n",
       "      <td>134.515195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>vid_4_9860.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.321729</td>\n",
       "      <td>49.235251</td>\n",
       "      <td>236.223284</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.901554</td>\n",
       "      <td>49.235251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>vid_4_9880.jpg</td>\n",
       "      <td>329.876184</td>\n",
       "      <td>156.482351</td>\n",
       "      <td>536.664239</td>\n",
       "      <td>250.497895</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.015544</td>\n",
       "      <td>206.788055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>vid_4_9900.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.295823</td>\n",
       "      <td>141.797524</td>\n",
       "      <td>239.176652</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.880829</td>\n",
       "      <td>141.797524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>vid_4_9960.jpg</td>\n",
       "      <td>487.428988</td>\n",
       "      <td>172.233646</td>\n",
       "      <td>616.917699</td>\n",
       "      <td>228.839864</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.606218</td>\n",
       "      <td>129.488711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>vid_4_9980.jpg</td>\n",
       "      <td>221.558631</td>\n",
       "      <td>182.570434</td>\n",
       "      <td>348.585579</td>\n",
       "      <td>238.192196</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.621762</td>\n",
       "      <td>127.026948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image        xmin        ymin        xmax        ymax  \\\n",
       "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547   \n",
       "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180   \n",
       "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180   \n",
       "4    vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422   \n",
       "5    vid_4_10100.jpg  447.568741  160.625804  582.083936  232.517696   \n",
       "..               ...         ...         ...         ...         ...   \n",
       "554   vid_4_9860.jpg    0.000000  198.321729   49.235251  236.223284   \n",
       "555   vid_4_9880.jpg  329.876184  156.482351  536.664239  250.497895   \n",
       "556   vid_4_9900.jpg    0.000000  168.295823  141.797524  239.176652   \n",
       "557   vid_4_9960.jpg  487.428988  172.233646  616.917699  228.839864   \n",
       "558   vid_4_9980.jpg  221.558631  182.570434  348.585579  238.192196   \n",
       "\n",
       "    class_name  has_annotation  image_id  class_id          h           w  \n",
       "0          car            True         0       0.0  36.190476   46.468886  \n",
       "1          car            True         1       0.0  49.395109  105.166425  \n",
       "2          car            True         3       0.0  59.665380  122.775687  \n",
       "4          car            True         4       0.0  51.840412  115.927641  \n",
       "5          car            True         6       0.0  71.891892  134.515195  \n",
       "..         ...             ...       ...       ...        ...         ...  \n",
       "554        car            True       994       0.0  37.901554   49.235251  \n",
       "555        car            True       995       0.0  94.015544  206.788055  \n",
       "556        car            True       996       0.0  70.880829  141.797524  \n",
       "557        car            True       999       0.0  56.606218  129.488711  \n",
       "558        car            True      1000       0.0  55.621762  127.026948  \n",
       "\n",
       "[397 rows x 11 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20926483-6282-4ab7-957c-bd14e0814d4a",
   "metadata": {},
   "source": [
    "As we will need to resize our images during training, these bounding boxes will also need to be resized. The easiest way for us to do this is to normalize these values, so then we can just multiply by our desired image size.\n",
    "\n",
    "However, to do this, we will need the height and width of our images. Sometimes, we have this information ahead of time, in which case we can use this knowledge directly. Otherwise, We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9975c3ff-ea78-4975-a327-93214babfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm.contrib.concurrent import process_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ffb3bf9d-a3d9-4f03-8471-5c5dc5f7debd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 324/324 [00:00<00:00, 4657.43it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_image_size(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    w, h = image.size\n",
    "    return (image_path.parts[-1], (w, h))\n",
    "\n",
    "image_sizes = process_map(find_image_size, [images_path/p for p in train_df.image.unique()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "835968e5-3f7a-4af0-98e5-050bc45f8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes_lookup = dict(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e370ecb4-1de7-48f3-ade7-bacada2b93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes_df = pd.DataFrame(image_sizes_lookup).T.reset_index().rename(columns={'index': 'image', 0: 'image_w', 1:'image_h'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3e9735ef-fc8e-459c-9993-3f4b1c6170e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>image_w</th>\n",
       "      <th>image_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10100.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>vid_4_13060.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>vid_4_13100.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>vid_4_13240.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>vid_4_13280.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>vid_4_13300.jpg</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image  image_w  image_h\n",
       "0     vid_4_1000.jpg      676      380\n",
       "1    vid_4_10000.jpg      676      380\n",
       "2    vid_4_10040.jpg      676      380\n",
       "3    vid_4_10060.jpg      676      380\n",
       "4    vid_4_10100.jpg      676      380\n",
       "..               ...      ...      ...\n",
       "319  vid_4_13060.jpg      676      380\n",
       "320  vid_4_13100.jpg      676      380\n",
       "321  vid_4_13240.jpg      676      380\n",
       "322  vid_4_13280.jpg      676      380\n",
       "323  vid_4_13300.jpg      676      380\n",
       "\n",
       "[324 rows x 3 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sizes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92edc7cb-b730-488b-b727-e7e96fb68c09",
   "metadata": {},
   "source": [
    "We can now merge this with our existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "804d03cb-f69f-46a8-81a8-1b20c977564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df = pd.merge(train_annotations_df, image_sizes_df, on='image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7724e-52cb-4abe-9ee5-198cd4d61985",
   "metadata": {},
   "source": [
    "We can now easily calulate our normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d44bd522-5b0f-4bd0-9b49-3dea76034fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_df['normalized_h'] = train_annotations_df['h']/train_annotations_df['image_h']\n",
    "train_annotations_df['normalized_w'] = train_annotations_df['w']/train_annotations_df['image_w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "02890621-7b59-44dd-bbfb-989a696e3370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class_name</th>\n",
       "      <th>has_annotation</th>\n",
       "      <th>image_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>image_w</th>\n",
       "      <th>image_h</th>\n",
       "      <th>normalized_h</th>\n",
       "      <th>normalized_w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vid_4_1000.jpg</td>\n",
       "      <td>281.259045</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>327.727931</td>\n",
       "      <td>223.225547</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.190476</td>\n",
       "      <td>46.468886</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.068741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vid_4_10000.jpg</td>\n",
       "      <td>15.163531</td>\n",
       "      <td>187.035071</td>\n",
       "      <td>120.329957</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.395109</td>\n",
       "      <td>105.166425</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.129987</td>\n",
       "      <td>0.155572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vid_4_10040.jpg</td>\n",
       "      <td>239.192475</td>\n",
       "      <td>176.764801</td>\n",
       "      <td>361.968162</td>\n",
       "      <td>236.430180</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.665380</td>\n",
       "      <td>122.775687</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.157014</td>\n",
       "      <td>0.181621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vid_4_10060.jpg</td>\n",
       "      <td>16.630970</td>\n",
       "      <td>186.546010</td>\n",
       "      <td>132.558611</td>\n",
       "      <td>238.386422</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.840412</td>\n",
       "      <td>115.927641</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.136422</td>\n",
       "      <td>0.171491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vid_4_10100.jpg</td>\n",
       "      <td>447.568741</td>\n",
       "      <td>160.625804</td>\n",
       "      <td>582.083936</td>\n",
       "      <td>232.517696</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.891892</td>\n",
       "      <td>134.515195</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.198987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>vid_4_9860.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>198.321729</td>\n",
       "      <td>49.235251</td>\n",
       "      <td>236.223284</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.901554</td>\n",
       "      <td>49.235251</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.099741</td>\n",
       "      <td>0.072833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>vid_4_9880.jpg</td>\n",
       "      <td>329.876184</td>\n",
       "      <td>156.482351</td>\n",
       "      <td>536.664239</td>\n",
       "      <td>250.497895</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.015544</td>\n",
       "      <td>206.788055</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.247409</td>\n",
       "      <td>0.305899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>vid_4_9900.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>168.295823</td>\n",
       "      <td>141.797524</td>\n",
       "      <td>239.176652</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.880829</td>\n",
       "      <td>141.797524</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.186528</td>\n",
       "      <td>0.209760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>vid_4_9960.jpg</td>\n",
       "      <td>487.428988</td>\n",
       "      <td>172.233646</td>\n",
       "      <td>616.917699</td>\n",
       "      <td>228.839864</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.606218</td>\n",
       "      <td>129.488711</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.148964</td>\n",
       "      <td>0.191551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>vid_4_9980.jpg</td>\n",
       "      <td>221.558631</td>\n",
       "      <td>182.570434</td>\n",
       "      <td>348.585579</td>\n",
       "      <td>238.192196</td>\n",
       "      <td>car</td>\n",
       "      <td>True</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.621762</td>\n",
       "      <td>127.026948</td>\n",
       "      <td>676</td>\n",
       "      <td>380</td>\n",
       "      <td>0.146373</td>\n",
       "      <td>0.187910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               image        xmin        ymin        xmax        ymax  \\\n",
       "0     vid_4_1000.jpg  281.259045  187.035071  327.727931  223.225547   \n",
       "1    vid_4_10000.jpg   15.163531  187.035071  120.329957  236.430180   \n",
       "2    vid_4_10040.jpg  239.192475  176.764801  361.968162  236.430180   \n",
       "3    vid_4_10060.jpg   16.630970  186.546010  132.558611  238.386422   \n",
       "4    vid_4_10100.jpg  447.568741  160.625804  582.083936  232.517696   \n",
       "..               ...         ...         ...         ...         ...   \n",
       "392   vid_4_9860.jpg    0.000000  198.321729   49.235251  236.223284   \n",
       "393   vid_4_9880.jpg  329.876184  156.482351  536.664239  250.497895   \n",
       "394   vid_4_9900.jpg    0.000000  168.295823  141.797524  239.176652   \n",
       "395   vid_4_9960.jpg  487.428988  172.233646  616.917699  228.839864   \n",
       "396   vid_4_9980.jpg  221.558631  182.570434  348.585579  238.192196   \n",
       "\n",
       "    class_name  has_annotation  image_id  class_id          h           w  \\\n",
       "0          car            True         0       0.0  36.190476   46.468886   \n",
       "1          car            True         1       0.0  49.395109  105.166425   \n",
       "2          car            True         3       0.0  59.665380  122.775687   \n",
       "3          car            True         4       0.0  51.840412  115.927641   \n",
       "4          car            True         6       0.0  71.891892  134.515195   \n",
       "..         ...             ...       ...       ...        ...         ...   \n",
       "392        car            True       994       0.0  37.901554   49.235251   \n",
       "393        car            True       995       0.0  94.015544  206.788055   \n",
       "394        car            True       996       0.0  70.880829  141.797524   \n",
       "395        car            True       999       0.0  56.606218  129.488711   \n",
       "396        car            True      1000       0.0  55.621762  127.026948   \n",
       "\n",
       "     image_w  image_h  normalized_h  normalized_w  \n",
       "0        676      380      0.095238      0.068741  \n",
       "1        676      380      0.129987      0.155572  \n",
       "2        676      380      0.157014      0.181621  \n",
       "3        676      380      0.136422      0.171491  \n",
       "4        676      380      0.189189      0.198987  \n",
       "..       ...      ...           ...           ...  \n",
       "392      676      380      0.099741      0.072833  \n",
       "393      676      380      0.247409      0.305899  \n",
       "394      676      380      0.186528      0.209760  \n",
       "395      676      380      0.148964      0.191551  \n",
       "396      676      380      0.146373      0.187910  \n",
       "\n",
       "[397 rows x 15 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369121eb-20a8-4585-99eb-6e617e0ef89d",
   "metadata": {},
   "source": [
    "### Calculating BPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407008e9-789b-4c16-b27c-95ed64e0d19c",
   "metadata": {},
   "source": [
    "Now that we have the normalized width and height of all ground truth boxes in our training set, we can evaluate our current anchor boxes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a6e1fa1a-adfc-4f16-a97b-30991a2dc80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor width and height multiple threshold used to select label-anchor matches when computing loss\n",
    "LOSS_ANCHOR_MULTIPLE_THRESHOLD = 4\n",
    "\n",
    "\n",
    "def calculate_best_possible_recall(anchors, normalized_gt_wh, image_sizes, target_image_size=640):\n",
    "    # image sizes array of [w, h] , either np.array([[w, h]]) or per image\n",
    "    \n",
    "     # find target image sizes, assuming resizing so that the longest side is the target size\n",
    "    target_image_sizes = (\n",
    "        target_image_size * image_sizes / image_sizes.max(1, keepdims=True)\n",
    "    )\n",
    "    \n",
    "    # find wh of boxes for target size\n",
    "    wh = target_image_sizes * normalized_gt_wh\n",
    "    \n",
    "    tiny_boxes_exist = (wh < 3).any(1).sum()\n",
    "    if tiny_boxes_exist:\n",
    "        print(\n",
    "            f\"WARNING: Extremely small objects found. {tiny_boxes_exist} of {len(wh)} labels are < 3 pixels in size.\"\n",
    "        )\n",
    "    \n",
    "    wh = wh[(wh >= 2.0).any(1)]  # filter > 2 pixels\n",
    "    \n",
    "    symmetric_size_ratios = torch.min(wh[:, None]/anchors[None], anchors[None]/wh[:, None]) # ensure 0-1 range\n",
    "    worst_side_size_ratio = symmetric_size_ratios.min(-1).values\n",
    "    best_anchor_ratio = worst_side_size_ratio.max(-1).values\n",
    "    best_possible_recall = (best_anchor_ratio > 1. / LOSS_ANCHOR_MULTIPLE_THRESHOLD).float().mean()\n",
    "    \n",
    "    return best_possible_recall\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "43a67b43-21da-4934-a972-899241879b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_gt_wh = train_annotations_df[['normalized_w', 'normalized_h']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6d970f9a-e3e2-4912-b975-d9f914730e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = train_annotations_df[['image_w', 'image_h']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f448be9d-8144-422d-9a10-e20cd9643b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_best_possible_recall(current_anchors, \n",
    "                                normalized_gt_wh,\n",
    "                                image_sizes\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "569bc528-df07-43cb-9bc7-f8862225751f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380],\n",
       "       [676, 380]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd444d2-97dd-4af9-bf27-fc5b0c4de2e6",
   "metadata": {},
   "source": [
    "Alternatively, as all of our images are the same size in this case, we could simply specify a single image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0a4c9027-cf77-4f6c-b05f-13264fb41911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "77e3ecb2-c6c5-4121-a539-99562df5ee5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_best_possible_recall(current_anchors, \n",
    "                                normalized_gt_wh,\n",
    "                                np.array([[676, 380]])\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46907396-a5ba-4d50-a3cb-9c9c0075974b",
   "metadata": {},
   "source": [
    "From this, we can see that the current anchor boxes are a good fit for this dataset; which makes sense, as the images are quite similar to those in COCO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf91ad-3654-47d7-9c35-b6bdb1137841",
   "metadata": {},
   "source": [
    "### How does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a3975-ab1c-4f16-88aa-1cbffb07113b",
   "metadata": {},
   "source": [
    "At this point, you may be wondering, how exactly do we calculate the best possible recall. To answer this, let's go through the process manually.\n",
    "\n",
    "First, we need to resize the width and height of our ground truth boxes based on the images that we are training on - for this architecture, this is recommended to be 640. To preserve the aspect ratios of the objects in our images, the recommended approach to resizing is to scale the image so that the longest size is equal to our target size. Following this approach, we can calculate the target size for each image as demonstrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf3ca7-4946-49e5-bd0b-13cbc19b24a1",
   "metadata": {},
   "source": [
    "#### Resize images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a396ee61-6422-44cf-9c8d-f843e3bac6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_size = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "35f786c2-98fe-4935-b820-cb410b67c87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[640.        , 359.76331361],\n",
       "       [640.        , 359.76331361],\n",
       "       [640.        , 359.76331361],\n",
       "       [640.        , 359.76331361],\n",
       "       [640.        , 359.76331361]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_image_sizes = (\n",
    "    target_image_size * image_sizes / image_sizes.max(1, keepdims=True)\n",
    "); target_image_sizes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27391be2-890e-4b27-9ca2-077df0c69672",
   "metadata": {},
   "source": [
    "Now that we have scaled our images, we also need to apply the same scaling to our ground truth labels. As these are normalized already, we can simply multiply these by our new image sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c862c362-2cf5-4f34-ac6b-d7ca1c1706a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_wh = target_image_sizes * normalized_gt_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5dbd24-31fa-47cb-97a2-4835c4c40942",
   "metadata": {},
   "source": [
    "Now comes the tricky bit, we would like to ensure that at least one anchor can be matches to each ground truth box. Whilst we could do this by framing it as an optimization problem - how do we match each ground truth box with its optimal anchor - this would introduce a lot of complexity for what we are trying to do.\n",
    "\n",
    "Given an anchor box, we need a simpler way of measuring how well it can be made to fit a ground truth box. Let's examine one approach that can be taken to do this, starting with the width and height of a single ground truth box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "634d2067-781e-4b48-ab83-85133eaf83b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43.99421122, 34.26317273])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_box_wh = gt_wh[0]; gt_box_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bba7a2-994f-4b70-a864-cbba735c6ee2",
   "metadata": {},
   "source": [
    "For each anchor box, we can inspect the ratios of its height and width when compared to the height and width of our ground truth target, and use this to understand where the biggest differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "06ab7b8b-fbdd-4af5-b34a-ef44249ea260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2728,  0.4670],\n",
       "        [ 0.4319,  1.0507],\n",
       "        [ 0.9092,  0.8172],\n",
       "        [ 0.8183,  2.1889],\n",
       "        [ 1.7275,  1.6052],\n",
       "        [ 1.6366,  4.2611],\n",
       "        [ 3.2277,  3.2104],\n",
       "        [ 4.3642,  7.0922],\n",
       "        [10.4332, 11.7035]], dtype=torch.float64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_anchors/gt_box_wh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8385ba-39e7-4994-a656-d9195d13bed0",
   "metadata": {},
   "source": [
    "As the scale of these ratios will depend on whether the anchor box sides are greater or smaller than the sides of our ground truth box, we can ensure that our magnitudes are in the range [0, 1] by also calculating the reciprocal and taking the minimum ratios for each anchor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e6042b81-de5a-42a7-a66c-c7d084f5b932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2728, 0.4670],\n",
       "        [0.4319, 0.9518],\n",
       "        [0.9092, 0.8172],\n",
       "        [0.8183, 0.4568],\n",
       "        [0.5789, 0.6230],\n",
       "        [0.6110, 0.2347],\n",
       "        [0.3098, 0.3115],\n",
       "        [0.2291, 0.1410],\n",
       "        [0.0958, 0.0854]], dtype=torch.float64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symmetric_size_ratios = torch.min(current_anchors/gt_box_wh, gt_box_wh/current_anchors); symmetric_size_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7d3ac-fc3f-499a-9189-59b42adce5a3",
   "metadata": {},
   "source": [
    "From this, we now have an indication of how well, independently, the width and height of each anchor box 'fits' to our ground truth box. \n",
    "\n",
    "Now, our challenge is how to consider the match of the the width and height together!\n",
    "\n",
    "One way we can approach this is, to take the minimum ratio for each anchor; representing the side that worst matches our ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c7a1e5c0-71ed-465e-813e-30f9feb9995e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2728, 0.4319, 0.8172, 0.4568, 0.5789, 0.2347, 0.3098, 0.1410, 0.0854],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_side_size_ratio = symmetric_size_ratios.min(-1).values; worst_side_size_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c655fb-a746-43f0-a217-4d74b075b46c",
   "metadata": {},
   "source": [
    "The reason why we have selected the worst fitting side here, is because we know that the other side matches our target *at least* as well as the one selected; we can think of this as the worst case scenario!\n",
    "\n",
    "Now, let's select the anchor box which matches the best out of these options, this is simply the largest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5fead607-04be-4537-b47b-f021147f184d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8172, dtype=torch.float64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_anchor_ratio = worst_side_size_ratio.max(-1).values; best_anchor_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf4414-92fd-43fd-b4c6-736370e524bf",
   "metadata": {},
   "source": [
    "Out of the worst fitting options, this is our selected match!\n",
    "\n",
    "\n",
    "TODO: Ensure this is after the loss function section\n",
    "\n",
    "Recalling that the loss function only looks to match anchor boxes that are within 4 times greater or smaller than the size of the ground truth target, we can now verify whether this anchor is within this range and would be considered a successful match.\n",
    "\n",
    "We can do that as demonstrated below, taking the reciprical of our loss multiple, to ensure that it is in the same range as our value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "950240a5-eeae-4141-bb23-6bd145e7bf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_anchor_ratio > 1. / ANCHOR_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de729b8-6bd7-450d-9668-35999b2c90af",
   "metadata": {},
   "source": [
    "From this, we can see that at least one of our anchors could be successfully matched to our selected ground truth target!\n",
    "\n",
    "Now that we understand the sequence of steps, we can now apply the same logic to all of our ground truth boxes to see how many matches we can obtain with our current set of anchors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "feab2b68-348c-4c64-8ff4-4a4a92d322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetric_size_ratios = torch.min(current_anchors[None]/gt_wh[:, None], gt_wh[:, None]/current_anchors[None])\n",
    "worst_side_size_ratio = symmetric_size_ratios.min(-1).values\n",
    "best_anchor_ratio = worst_side_size_ratio.max(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2e5240a5-2d88-4ed9-a483-7501790d804a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_anchor_ratio > 1. / ANCHOR_THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0c5d5-8c56-4907-85c8-8f48bd1f1e46",
   "metadata": {},
   "source": [
    "Now that we have calculated, for each ground truth box, whether it has a match. We can take the mean number of matches to find out best possible recall; in our case, this is 1, as we saw earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "33a55c5f-cb1b-4003-9abc-c04ca1ddf384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_possible_recall = (best_anchor_ratio > 1. / ANCHOR_THRESHOLD).float().mean(); best_possible_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adc050-5895-4c0c-96a2-6ba3df302d13",
   "metadata": {},
   "source": [
    "Whilst we have tried to match an anchor to every ground truth box in the training set for this example, in practice, we often filter out incredibly small boxes (less than 3 pixels in either height or width), as these boxes are usually too small to be considered useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13a225-a0f7-4f3a-a69e-3766628eeb0d",
   "metadata": {},
   "source": [
    "## Selecting new anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625a535-6c8a-46ff-99c5-54cf4ea470f3",
   "metadata": {},
   "source": [
    "Whilst using the pre-defined anchors may be a good choice for similar datasets, this may not be appropriate for all datasets, for example, those that contain lots of small objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66b7dc-0b74-40d6-a371-c79ea2cad532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
